{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"data.ipynb","provenance":[],"collapsed_sections":["LbLh66vHHxYp"],"mount_file_id":"19facTKth8ssee2HCtntaUumwwsyz8I16","authorship_tag":"ABX9TyPYaLMy7yGz6QwzakNHySs0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_AzNdOTBU09F","executionInfo":{"status":"ok","timestamp":1650011103368,"user_tz":-180,"elapsed":7,"user":{"displayName":"Afshan Nabi","userId":"10688478364672203261"}},"outputId":"6e8dbbc4-e601-438c-8a1c-ec69b9804870"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"Eme4ROlgKkHN","executionInfo":{"status":"error","timestamp":1650016642106,"user_tz":-180,"elapsed":407,"user":{"displayName":"Afshan Nabi","userId":"10688478364672203261"}},"outputId":"461477f1-797d-45a9-bbbc-1fece0c4a63e"},"outputs":[{"output_type":"stream","name":"stderr","text":["usage: ipykernel_launcher.py [-h] -i READ_DIR -o WRITE_DIR\n","ipykernel_launcher.py: error: the following arguments are required: -i/--read_dir, -o/--write_dir\n"]},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"]}],"source":["import argparse\n","import itertools\n","import numpy as np\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","import pickle\n","import pathlib\n","\n","# truncate sequences longer than max_len\n","MAX_LEN = 4000\n","KMER = 3 # or 6\n","\n","# read_dir = pathlib.Path('.../misannotated_lncrna/data/external/cppred')\n","# write_dir = pathlib.Path('.../misannotated_lncrna/data/interim')\n","\n","def tokenize_sequences(sequences, kmer=KMER, max_len=MAX_LEN):\n","    \"\"\"\n","    takes input sequences e.g. \"AAAAAC\" and returns tokenized representation.\n","    \n","    i've implemented a sliding window kmer tokenization, i.e. kmer window shifts \n","    by 1 at each step. Therefore, 3-mer tokens of AAAAAC would be \n","    [AAA, AAA, AAA, AAC] \n","\n","    (at some point, i'd like to compare to the discrete tokenization as well, \n","    i.e the one where 3-mer tokens of AAAAAC would be [AAA, AAC] only)\n","\n","    e.g. if tokernizer is {AAA:1, AAC: 2, AAG:3, ...}, then, \n","    \"AAAAAC\" -> [AAA, AAA, AAA, AAC] -> [1,1,1,2]\n","\n","    Most of this function has been adapted from \n","    https://github.com/hzy95/EPIVAN/blob/master/sequence_processing.py\n","\n","    \"\"\"\n"," \n","    def sentence2word(str_set, k_mer=kmer):\n","        word_seq=[]\n","        for sr in str_set:\n","            tmp=[]\n","            for i in range(len(sr)-(k_mer-1)):\n","                tmp.append(sr[i:i+k_mer])\n","            word_seq.append(' '.join(tmp))\n","        return word_seq\n","\n","    def word2num(wordseq, tokenizer, max_len):\n","        sequences = tokenizer.texts_to_sequences(wordseq)\n","        numseq = pad_sequences(sequences, maxlen=max_len)\n","        return numseq\n","\n","    def sentence2num(str_set, tokenizer, max_len):\n","        wordseq = sentence2word(str_set)\n","        numseq = word2num(wordseq, tokenizer, max_len)\n","        return numseq\n","\n","    def get_tokenizer(k_mer):\n","        f= ['a','c','g','t']\n","        res=[]\n","\n","        if k_mer == 3:\n","            c = itertools.product(f,f,f)\n","            for i in c:\n","                temp=i[0]+i[1]+i[2]\n","                res.append(temp)\n","\n","        elif k_mer == 6:\n","            c = itertools.product(f,f,f,f,f,f)\n","            for i in c:\n","                temp=i[0]+i[1]+i[2]+i[3]+i[4]+i[5]\n","                res.append(temp)    \n","        \n","        res=np.array(res)\n","        NB_WORDS = res.shape[0]\n","        tokenizer = Tokenizer(num_words=NB_WORDS)\n","        tokenizer.fit_on_texts(res)\n","        acgt_index = tokenizer.word_index\n","        acgt_index['null'] = 0\n","        return tokenizer\n","\n","    tokenizer = get_tokenizer(kmer)\n","    tokenized_sequences = sentence2num(sequences, tokenizer, max_len)\n","    return tokenized_sequences\n","\n","\n","def main(read_dir, write_dir):\n","    # test- coding rnas\n","    crna_test = open(read_dir / 'Human_coding_RNA_test.fa','r').read().splitlines()\n","    crna_test_seqs = crna_test[1::2]\n","    crna_test_ids = [s.split('|')[3] for s in crna_test[0::2]]\n","\n","    crna_test_tokens = tokenize_sequences(crna_test_seqs, KMER, MAX_LEN)\n","\n","    with open(write_dir / 'X_c_test.pickle', 'wb') as handle:\n","        pickle.dump(crna_test_tokens, handle)\n","    with open(write_dir / 'X_c_ids_test.pickle', 'wb') as handle:\n","        pickle.dump(crna_test_ids, handle)\n","\n","    # test- noncoding rnas\n","    ncrna_test = open(read_dir / 'Homo38_ncrna_test.fa','r').read().splitlines()\n","    ncrna_test_seqs = ncrna_test[1::2]\n","    lnc_seqs_inds =  [i for i,seq in enumerate(ncrna_test_seqs) if len(seq)>200]\n","    ncrna_test_seqs_lnc = [ncrna_test_seqs[i] for i in lnc_seqs_inds]\n","    ncrna_test_ids = [s.split(' ')[0][1:] for s in ncrna_test[0::2]]\n","    ncrna_test_ids_lnc = [ncrna_test_ids[i] for i in lnc_seqs_inds]\n","\n","    ncrna_test_tokens = tokenize_sequences(ncrna_test_seqs_lnc, KMER, MAX_LEN)\n","\n","    with open(write_dir / 'X_nc_test.pickle', 'wb') as handle:\n","        pickle.dump(ncrna_test_tokens, handle)\n","    with open(write_dir / 'X_nc_ids_test.pickle', 'wb') as handle:\n","        pickle.dump(ncrna_test_ids_lnc, handle)\n","\n","    # train- coding rnas\n","    crna_train = open(read_dir / 'Human.coding_RNA_training.fa','r').read().splitlines()\n","    crna_train_seqs = crna_train[1::2]\n","    crna_train_ids = [s.split('|')[3] for s in crna_train[0::2]]\n","\n","    crna_train_tokens = tokenize_sequences(crna_train_seqs, KMER, MAX_LEN)\n","\n","    with open(write_dir / 'X_c_train.pickle', 'wb') as handle:\n","        pickle.dump(crna_train_tokens, handle)\n","    with open(write_dir / 'X_c_ids_train.pickle', 'wb') as handle:\n","        pickle.dump(crna_train_ids, handle)\n","\n","\n","    # train- noncoding rnas\n","    ncrna_train = open(read_dir / 'Homo38.ncrna_training.fa','r').read().splitlines()\n","    ncrna_train_seqs = ncrna_train[1::2]\n","    lnc_seqs_inds =  [i for i,seq in enumerate(ncrna_train_seqs) if len(seq)>200]\n","    ncrna_train_seqs_lnc = [ncrna_train_seqs[i] for i in lnc_seqs_inds]\n","    ncrna_train_ids = [s.split(' ')[0][1:] for s in ncrna_train[0::2]]\n","    ncrna_train_ids_lnc = [ncrna_train_ids[i] for i in lnc_seqs_inds]\n","\n","    ncrna_train_tokens = tokenize_sequences(ncrna_train_seqs_lnc, KMER, MAX_LEN)\n","\n","    with open(write_dir / 'X_nc_train.pickle', 'wb') as handle:\n","        pickle.dump(ncrna_train_tokens, handle)\n","    with open(write_dir / 'X_nc_ids_train.pickle', 'wb') as handle:\n","        pickle.dump(ncrna_train_ids_lnc, handle)\n","\n","\n","def parse_args():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\n","        \"-i\",\n","        \"--read_dir\",\n","        help=\"Path to data to read in\",\n","        required=True,\n","    )\n","    parser.add_argument(\n","        \"-o\",\n","        \"--write_dir\",\n","        help=\"Path to where new data should be written\",\n","        required=True,\n","    )\n","    return parser.parse_args()\n","\n","\n","if __name__ == \"__main__\":\n","    args = parse_args()\n","    main(\n","        read_dir=pathlib.Path(args.read_dir),\n","        write_dir=pathlib.Path(args.write_dir),\n","    )"]},{"cell_type":"code","source":["!python3 src/data/preprocess.py -i data/external/cppred -o data/temp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0dgK47BuqBus","executionInfo":{"status":"ok","timestamp":1650090916683,"user_tz":-180,"elapsed":108139,"user":{"displayName":"Afshan Nabi","userId":"10688478364672203261"}},"outputId":"6f1a3f21-3229-44ba-8eb8-a66800b82dbe"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Preprocessing test coding RNAs...\n","Preprocessing test non-coding RNAs...\n","Preprocessing train coding RNAs...\n","Preprocessing train coding RNAs...\n"]}]},{"cell_type":"code","source":["!rm -rf data/temp"],"metadata":{"id":"j3u38mXiFXwL","executionInfo":{"status":"ok","timestamp":1650090947710,"user_tz":-180,"elapsed":414,"user":{"displayName":"Afshan Nabi","userId":"10688478364672203261"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# dna2vec"],"metadata":{"id":"LbLh66vHHxYp"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive/00Projects/lncrna/misannotated_lncrna\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hy9Jkl_EKcpQ","executionInfo":{"status":"ok","timestamp":1650025180722,"user_tz":-180,"elapsed":251,"user":{"displayName":"Afshan Nabi","userId":"10688478364672203261"}},"outputId":"68929dd4-c1cd-438b-922a-c60eaa4da39d"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/00Projects/lncrna/misannotated_lncrna\n","data\t Makefile   README.md\trequirements.txt  src\n","docs\t models     references\tsample_runs.txt   test_environment.py\n","LICENSE  notebooks  reports\tsetup.py\t  tox.ini\n"]}]},{"cell_type":"code","source":["import itertools\n","import numpy as np\n","from keras.preprocessing.text import Tokenizer\n","import pickle  \n","import pathlib\n","\n","\n","# read_dir = pathlib.Path('/data/external/dna2vec')\n","# write_dir = pathlib.Path('/data/interim')\n","\n","\n","def getKmerEmbeddingMatrix(read_dir: pathlib.Path, kmer: int):\n","    \"\"\"\n","    read_dir: path at which w2v file for dna embeddings is located\n","        dna2vec representations are obtained from https://arxiv.org/abs/1701.06279\n","    kmer: can be either 3 or 6, length of kmer for which to get embedding\n","    \"\"\"  \n","    embedding_dim = 100\n","    # Read all embeddings\n","    embeddings_index = {}\n","    f = open(read_dir / 'dna2vec-20161219-0153-k3to8-100d-10c-29320Mbp-sliding-Xat.w2v')\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        embeddings_index[word] = coefs\n","    f.close()\n","    # Get word indices for relevant kmer\n","    f= ['a','c','g','t']\n","    res=[]\n","    if kmer == 3:    \n","        c = itertools.product(f,f,f)    \n","        for i in c:\n","            temp=i[0]+i[1]+i[2]\n","            res.append(temp)\n","    elif kmer == 6:    \n","        c = itertools.product(f,f,f,f,f,f)    \n","        for i in c:\n","            temp=i[0]+i[1]+i[2]+i[3]+i[4]+i[5]\n","            res.append(temp)\n","\n","    res=np.array(res)\n","    NB_WORDS = res.shape[0] + 1\n","    tokenizer = Tokenizer(num_words=NB_WORDS)\n","    tokenizer.fit_on_texts(res)\n","    word_index = tokenizer.word_index\n","    word_index['null']=0\n","\n","    # get embedding matrix\n","    embedding_matrix = np.zeros((len(word_index), embedding_dim))\n","    for word, i in word_index.items(): \n","        embedding_vector = embeddings_index.get(word.upper())\n","        if embedding_vector is not None:\n","            # words not found in embedding index will be all-zeros.\n","            embedding_matrix[i] = embedding_vector\n","    return embedding_matrix\n","\n","def main(read_dir, write_dir):\n","    embedding_matrix_6mer = getKmerEmbeddingMatrix(read_dir, 6)\n","    embedding_matrix_3mer = getKmerEmbeddingMatrix(read_dir, 3)\n","\n","    with open(write_dir / 'embedding_matrix_6mer.pickle', 'wb') as handle:\n","        pickle.dump(embedding_matrix_6mer, handle)\n","    with open(write_dir / 'embedding_matrix_3mer.pickle', 'wb') as handle:\n","        pickle.dump(embedding_matrix_3mer, handle)\n","\n","def parse_args():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\n","        \"-i\",\n","        \"--read_dir\",\n","        help=\"Path to dna2vec file\",\n","        required=True,\n","    )\n","    parser.add_argument(\n","        \"-o\",\n","        \"--write_dir\",\n","        help=\"Path to where kmer embeddings should be written\",\n","        required=True,\n","    )\n","    return parser.parse_args()\n","\n","\n","if __name__ == \"__main__\":\n","    args = parse_args()\n","    main(\n","        read_dir=pathlib.Path(args.read_dir),\n","        write_dir=pathlib.Path(args.write_dir),\n","    )"],"metadata":{"id":"eSDGHWi8rkgP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python3 src/data/get_embeddings.py -i data/external/dna2vec/ -o data/temp/"],"metadata":{"id":"diLfGUcVLBS8","executionInfo":{"status":"ok","timestamp":1650090719967,"user_tz":-180,"elapsed":7270,"user":{"displayName":"Afshan Nabi","userId":"10688478364672203261"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["# data_preprocessing_utils"],"metadata":{"id":"iiHFCx-tPvAH"}},{"cell_type":"code","source":["def tokenizerGetWordIndex(kmerLen = 3):\n","    '''\n","    Returns kmer length words w/ associated indices\n","    '''\n","    \n","    f= ['a','c','g','t']\n","    res=[]\n","\n","    if kmerLen == 6:\n","        c = itertools.product(f,f,f,f,f,f)\n","        for i in c:\n","            temp=i[0]+i[1]+i[2]+i[3]+i[4]+i[5]\n","            res.append(temp)\n","    elif kmerLen == 3:\n","        c = itertools.product(f,f,f)\n","        for i in c:\n","            temp=i[0]+i[1]+i[2]\n","            res.append(temp)\n","    \n","    res=np.array(res)\n","    NB_WORDS = len(res) + 1\n","    tokenizer = Tokenizer(num_words=NB_WORDS)\n","    tokenizer.fit_on_texts(res)\n","    word_index = tokenizer.word_index\n","    word_index['null']=0\n","\n","    return word_index\n","\n","def readEmbeddingMatrix(kmer, dataDir):\n","    '''\n","    Read embedding matrix for relevant kmer\n","    '''\n","    if kmer == '6mer':\n","        with open(dataDir + 'embedding_matrix_6mer.pickle', 'rb') as handle:\n","            embedding_matrix = pickle.load(handle)\n","    elif kmer == '3mer':\n","        with open(dataDir + 'embedding_matrix_3mer.pickle', 'rb') as handle:\n","            embedding_matrix = pickle.load(handle)\n","    return embedding_matrix"],"metadata":{"id":"kC-gqCciPz6c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# train and valid data splits"],"metadata":{"id":"_hRHoooFPp3g"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive/00Projects/lncrna/misannotated_lncrna"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o5WuSx63LUpV","executionInfo":{"status":"ok","timestamp":1650089439052,"user_tz":-180,"elapsed":386,"user":{"displayName":"Afshan Nabi","userId":"10688478364672203261"}},"outputId":"bbfe7e2b-05ea-4be2-8f9a-b5927244138d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/00Projects/lncrna/misannotated_lncrna\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/drive/MyDrive/00Projects/lncrna/misannotated_lncrna')"],"metadata":{"id":"j-EDekx8WgiJ","executionInfo":{"status":"ok","timestamp":1650028330514,"user_tz":-180,"elapsed":241,"user":{"displayName":"Afshan Nabi","userId":"10688478364672203261"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h-lTu2TuXmJJ","executionInfo":{"status":"ok","timestamp":1650028606401,"user_tz":-180,"elapsed":406,"user":{"displayName":"Afshan Nabi","userId":"10688478364672203261"}},"outputId":"34885ac4-dde0-4f99-84df-669151f3b86d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["data\t Makefile   README.md\trequirements.txt  src\n","docs\t models     references\tsample_runs.txt   test_environment.py\n","LICENSE  notebooks  reports\tsetup.py\t  tox.ini\n"]}]},{"cell_type":"code","source":["from src.data.data_utils import tokenizerGetWordIndex"],"metadata":{"id":"aRbJgVBKWVsL","executionInfo":{"status":"ok","timestamp":1650028353325,"user_tz":-180,"elapsed":3650,"user":{"displayName":"Afshan Nabi","userId":"10688478364672203261"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["tokenizerGetWordIndex"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qwKDu0zPWodf","executionInfo":{"status":"ok","timestamp":1650028358047,"user_tz":-180,"elapsed":258,"user":{"displayName":"Afshan Nabi","userId":"10688478364672203261"}},"outputId":"f9fc3160-7e5e-4fa2-9369-085e6d3f4c85"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<function src.data.data_utils.tokenizerGetWordIndex>"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["!python3 src/data/trial.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TIb8Ek1dWqcf","executionInfo":{"status":"ok","timestamp":1650089890959,"user_tz":-180,"elapsed":3229,"user":{"displayName":"Afshan Nabi","userId":"10688478364672203261"}},"outputId":"85524728-4df5-4aec-e5e4-62e7cfc3c674"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["hello this works\n"]}]},{"cell_type":"code","source":["import itertools\n","import numpy as np\n","from keras.preprocessing.text import Tokenizer\n","import pickle  \n","import pathlib\n","\n","\n","def get_tokenizer(k_mer):\n","        f= ['a','c','g','t']\n","        res=[]\n","\n","        if k_mer == 3:\n","            c = itertools.product(f,f,f)\n","            for i in c:\n","                temp=i[0]+i[1]+i[2]\n","                res.append(temp)\n","\n","        elif k_mer == 6:\n","            c = itertools.product(f,f,f,f,f,f)\n","            for i in c:\n","                temp=i[0]+i[1]+i[2]+i[3]+i[4]+i[5]\n","                res.append(temp)    \n","        \n","        res=np.array(res)\n","        NB_WORDS = res.shape[0]\n","        tokenizer = Tokenizer(num_words=NB_WORDS)\n","        tokenizer.fit_on_texts(res)\n","        acgt_index = tokenizer.word_index\n","        acgt_index['null'] = 0\n","        return tokenizer\n","\n","t = get_tokenizer(3)\n","t"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rhDOyI_VXAS2","executionInfo":{"status":"ok","timestamp":1650090453013,"user_tz":-180,"elapsed":3157,"user":{"displayName":"Afshan Nabi","userId":"10688478364672203261"}},"outputId":"66b838e6-b7da-4f88-c4c2-68c97ab3fd82"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<keras_preprocessing.text.Tokenizer at 0x7f53d3590090>"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["dir(t)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rceiuB7tDbuZ","executionInfo":{"status":"ok","timestamp":1650090463804,"user_tz":-180,"elapsed":5,"user":{"displayName":"Afshan Nabi","userId":"10688478364672203261"}},"outputId":"6ccdcc70-33e5-43e9-a0b5-5cf353bef0ca"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['__class__',\n"," '__delattr__',\n"," '__dict__',\n"," '__dir__',\n"," '__doc__',\n"," '__eq__',\n"," '__format__',\n"," '__ge__',\n"," '__getattribute__',\n"," '__gt__',\n"," '__hash__',\n"," '__init__',\n"," '__init_subclass__',\n"," '__le__',\n"," '__lt__',\n"," '__module__',\n"," '__ne__',\n"," '__new__',\n"," '__reduce__',\n"," '__reduce_ex__',\n"," '__repr__',\n"," '__setattr__',\n"," '__sizeof__',\n"," '__str__',\n"," '__subclasshook__',\n"," '__weakref__',\n"," '_keras_api_names',\n"," '_keras_api_names_v1',\n"," 'char_level',\n"," 'document_count',\n"," 'filters',\n"," 'fit_on_sequences',\n"," 'fit_on_texts',\n"," 'get_config',\n"," 'index_docs',\n"," 'index_word',\n"," 'lower',\n"," 'num_words',\n"," 'oov_token',\n"," 'sequences_to_matrix',\n"," 'sequences_to_texts',\n"," 'sequences_to_texts_generator',\n"," 'split',\n"," 'texts_to_matrix',\n"," 'texts_to_sequences',\n"," 'texts_to_sequences_generator',\n"," 'to_json',\n"," 'word_counts',\n"," 'word_docs',\n"," 'word_index']"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":[""],"metadata":{"id":"qQ0RP1R9Dk-t"},"execution_count":null,"outputs":[]}]}